{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqbwsNhFqDF+IwN4IBECCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zrghassabi/System_Design_Coding/blob/main/accountEmbeddingsInInstagram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Goal:\n",
        "\n",
        "Train embeddings of account IDs from user sessions using Skip-gram with negative sampling."
      ],
      "metadata": {
        "id": "-e556BFsGVJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üì¶ Required Libraries"
      ],
      "metadata": {
        "id": "o3KnedhOGRFq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO8XirHvEe17"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üßæ Sample Sessions"
      ],
      "metadata": {
        "id": "PxL4_4FVGMsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated user sessions\n",
        "sessions = [\n",
        "    [\"catsdaily\", \"meowworld\", \"funnycats\"],\n",
        "    [\"travelgram\", \"naturepics\", \"worldexplorer\"],\n",
        "    [\"catsdaily\", \"catvideos\", \"meowworld\"],\n",
        "    [\"worldexplorer\", \"earthpix\", \"travelgram\"]\n",
        "]\n",
        "\n",
        "# Build vocabulary\n",
        "all_accounts = list(set(acc for session in sessions for acc in session))\n",
        "word2idx = {w: idx for idx, w in enumerate(all_accounts)}\n",
        "idx2word = {idx: w for w, idx in word2idx.items()}\n",
        "vocab_size = len(all_accounts)\n",
        "\n",
        "print(all_accounts)\n",
        "print(word2idx)\n",
        "print(idx2word)\n",
        "print(vocab_size)\n"
      ],
      "metadata": {
        "id": "lmle7i69FKdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Skip-gram Dataset Generator"
      ],
      "metadata": {
        "id": "nl-Al0Z2GG0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_skipgram_data(sessions, window_size=1):\n",
        "    pairs = []\n",
        "    for session in sessions:\n",
        "        for center_pos in range(len(session)):\n",
        "            for offset in range(-window_size, window_size + 1):\n",
        "                context_pos = center_pos + offset\n",
        "                if context_pos < 0 or context_pos >= len(session) or context_pos == center_pos:\n",
        "                    continue\n",
        "                center = word2idx[session[center_pos]]\n",
        "                context = word2idx[session[context_pos]]\n",
        "                pairs.append((center, context))\n",
        "    return pairs\n",
        "\n",
        "print(generate_skipgram_data(sessions,1))"
      ],
      "metadata": {
        "id": "zBOd7aQuFMMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Simple Embedding Model (Skip-gram with Negative Sampling)"
      ],
      "metadata": {
        "id": "dZ8-Om96GDGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.input_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.output_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, center, context, negs):\n",
        "        center_emb = self.input_embeddings(center)  # (batch, dim)\n",
        "        context_emb = self.output_embeddings(context)  # (batch, dim)\n",
        "        neg_emb = self.output_embeddings(negs)  # (batch, neg_samples, dim)\n",
        "\n",
        "        pos_score = torch.sum(center_emb * context_emb, dim=1)  # dot product\n",
        "        #neg_score = torch.bmm(neg_emb, center_emb.unsqueeze(2)).squeeze()\n",
        "        neg_score = torch.bmm(neg_emb, center_emb.unsqueeze(2)).squeeze(2)  # shape: (batch, neg_samples)\n",
        "        loss = -torch.log(torch.sigmoid(pos_score)) - torch.sum(torch.log(torch.sigmoid(-neg_score)), dim=1)\n",
        "        #loss = -torch.log(torch.sigmoid(pos_score)) - torch.sum(torch.log(torch.sigmoid(-neg_score)), dim=1)\n",
        "        return loss.mean()\n"
      ],
      "metadata": {
        "id": "XS5-9klmGB1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÅ Training"
      ],
      "metadata": {
        "id": "v3QjdZpPGz21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 2\n",
        "model = SkipGramModel(vocab_size, embed_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Generate training pairs\n",
        "pairs = generate_skipgram_data(sessions)\n",
        "num_epochs = 100\n",
        "neg_samples = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    random.shuffle(pairs)\n",
        "    for center, context in pairs:\n",
        "        negs = [random.randint(0, vocab_size - 1) for _ in range(neg_samples)]\n",
        "        center_tensor = torch.tensor([center])\n",
        "        context_tensor = torch.tensor([context])\n",
        "        neg_tensor = torch.tensor([negs])\n",
        "\n",
        "        loss = model(center_tensor, context_tensor, neg_tensor)\n",
        "        total_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "28yLpbwfG0ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üì§ View the Learned Embeddings"
      ],
      "metadata": {
        "id": "Qgur0lIKHCsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.input_embeddings.weight.data\n",
        "for idx, vec in enumerate(embeddings):\n",
        "    print(f\"{idx2word[idx]} ‚Üí {vec.numpy()}\")\n"
      ],
      "metadata": {
        "id": "lluJozW6HFAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Step 1: Visualize Embeddings in 2D"
      ],
      "metadata": {
        "id": "75yxFIU-HMgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get embedding weights\n",
        "embeddings = model.input_embeddings.weight.data\n",
        "\n",
        "# Plot each point\n",
        "plt.figure(figsize=(8, 6))\n",
        "for idx, vec in enumerate(embeddings):\n",
        "    x, y = vec.numpy()\n",
        "    plt.scatter(x, y)\n",
        "    plt.text(x + 0.01, y + 0.01, idx2word[idx], fontsize=12)\n",
        "\n",
        "plt.title(\"Account Embeddings (ig2vec-style)\")\n",
        "plt.xlabel(\"Dim 1\")\n",
        "plt.ylabel(\"Dim 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d-yNde1mHNbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ This will show accounts like @catsdaily, @meowworld clustered together if the model worked well ‚Äî indicating topical similarity."
      ],
      "metadata": {
        "id": "4olQRiBZHSbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß≠ Step 2: Find Similar Accounts (Cosine Similarity)"
      ],
      "metadata": {
        "id": "swbC0EDwHWNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def get_similar_accounts(account_name, top_k=3):\n",
        "    account_id = word2idx[account_name]\n",
        "    target_vec = embeddings[account_id].unsqueeze(0)  # shape (1, dim)\n",
        "    print(f\" account id {account_id}, target_vec {target_vec},\\n embed \\n {embeddings}\")\n",
        "\n",
        "    # Normalize all embeddings and compute cosine similarity\n",
        "    norms = F.normalize(embeddings, dim=1)\n",
        "    print(f\"noms  : \\n  {norms}\")\n",
        "    sim = torch.matmul(F.normalize(target_vec, dim=1), norms.T).squeeze()\n",
        "\n",
        "    # Get top k similar accounts (excluding self)\n",
        "    topk = torch.topk(sim, top_k + 1)\n",
        "    for i, idx in enumerate(topk.indices):\n",
        "        if idx == account_id:\n",
        "            continue\n",
        "        print(f\"{i}. {idx2word[idx.item()]} ‚Üí similarity: {sim[idx]:.4f}\")\n"
      ],
      "metadata": {
        "id": "srhoskELHW32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ñ∂Ô∏è Example usage:"
      ],
      "metadata": {
        "id": "C4n_OI2NHbmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_similar_accounts(\"catsdaily\", top_k=3)\n"
      ],
      "metadata": {
        "id": "SZGq5NjvHcp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function returns the most similar accounts to \"catsdaily\" based on learned embeddings ‚Äî just like Instagram uses to find similar content sources."
      ],
      "metadata": {
        "id": "e23HG9IIHjAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîß Part 1: Improve Negative Sampling\n",
        "\n",
        "Instead of choosing random negatives (which may be too easy), we‚Äôll sample harder negatives that are:\n",
        "\n",
        "    Not in the same session as the current center\n",
        "\n",
        "    Appear frequently in other sessions (more informative)"
      ],
      "metadata": {
        "id": "lfvCX3jfXk_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÅ Step 1: Improve Negative Sampling"
      ],
      "metadata": {
        "id": "imVWh12tXw-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Build frequency counter of all accounts\n",
        "account_freq = Counter()\n",
        "for session in sessions:\n",
        "    for acc in session:\n",
        "        account_freq[word2idx[acc]] += 1\n",
        "\n",
        "def get_hard_negative_samples(center_id, session_ids, k=3):\n",
        "    all_ids = set(range(vocab_size))\n",
        "    context_ids = set(session_ids)\n",
        "    candidate_neg_ids = list(all_ids - context_ids - {center_id})\n",
        "\n",
        "    if len(candidate_neg_ids) < k:\n",
        "        # Fallback to random sampling from all non-center accounts\n",
        "        candidate_neg_ids = list(all_ids - {center_id})\n",
        "        k = min(len(candidate_neg_ids), k)\n",
        "\n",
        "    # Sort by frequency (optional for harder negatives)\n",
        "    candidate_neg_ids.sort(key=lambda x: -account_freq[x])\n",
        "\n",
        "    return random.sample(candidate_neg_ids, k)\n",
        "\n"
      ],
      "metadata": {
        "id": "_cF4hH23HTNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Replace the old random.randint() sampling inside training loop with:"
      ],
      "metadata": {
        "id": "M4-Q2jS0YKnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 2\n",
        "model = SkipGramModel(vocab_size, embed_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Generate training pairs\n",
        "pairs = generate_skipgram_data(sessions)\n",
        "num_epochs = 100\n",
        "neg_samples = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    random.shuffle(pairs)\n",
        "    for center, context in pairs:\n",
        "        #negs = [random.randint(0, vocab_size - 1) for _ in range(neg_samples)]\n",
        "        negs = get_hard_negative_samples(center, [w for w, _ in pairs if w != center])\n",
        "        center_tensor = torch.tensor([center])\n",
        "        context_tensor = torch.tensor([context])\n",
        "        neg_tensor = torch.tensor([negs])\n",
        "\n",
        "        loss = model(center_tensor, context_tensor, neg_tensor)\n",
        "        total_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "URp3lSNjYLeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Now, negative examples are more meaningful, helping the model learn better embeddings."
      ],
      "metadata": {
        "id": "ZHydlCm-Ympv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.input_embeddings.weight.data\n",
        "for idx, vec in enumerate(embeddings):\n",
        "    print(f\"{idx2word[idx]} ‚Üí {vec.numpy()}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get embedding weights\n",
        "embeddings = model.input_embeddings.weight.data\n",
        "\n",
        "# Plot each point\n",
        "plt.figure(figsize=(8, 6))\n",
        "for idx, vec in enumerate(embeddings):\n",
        "    x, y = vec.numpy()\n",
        "    plt.scatter(x, y)\n",
        "    plt.text(x + 0.01, y + 0.01, idx2word[idx], fontsize=12)\n",
        "\n",
        "plt.title(\"Account Embeddings (ig2vec-style)\")\n",
        "plt.xlabel(\"Dim 1\")\n",
        "plt.ylabel(\"Dim 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_similar_accounts(account_name, top_k=3):\n",
        "    account_id = word2idx[account_name]\n",
        "    target_vec = embeddings[account_id].unsqueeze(0)  # shape (1, dim)\n",
        "    print(f\" account id {account_id}, target_vec {target_vec},\\n embed \\n {embeddings}\")\n",
        "\n",
        "    # Normalize all embeddings and compute cosine similarity\n",
        "    norms = F.normalize(embeddings, dim=1)\n",
        "    print(f\"noms  : \\n  {norms}\")\n",
        "    sim = torch.matmul(F.normalize(target_vec, dim=1), norms.T).squeeze()\n",
        "\n",
        "    # Get top k similar accounts (excluding self)\n",
        "    topk = torch.topk(sim, top_k + 1)\n",
        "    for i, idx in enumerate(topk.indices):\n",
        "        if idx == account_id:\n",
        "            continue\n",
        "        print(f\"{i}. {idx2word[idx.item()]} ‚Üí similarity: {sim[idx]:.4f}\")\n",
        "\n",
        "\n",
        "get_similar_accounts(\"catsdaily\", top_k=3)"
      ],
      "metadata": {
        "id": "0psy4xkyiwe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üå± Part 2: Use Seed Accounts for Explore Recommendations\n",
        "üîé Step 2: Find Seed Accounts"
      ],
      "metadata": {
        "id": "bdwRkCORYq8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_seed_accounts(user_sessions):\n",
        "    \"\"\"Return unique recent accounts a user interacted with\"\"\"\n",
        "    last_session = user_sessions[-1]  # most recent session\n",
        "    return [word2idx[acc] for acc in last_session]\n"
      ],
      "metadata": {
        "id": "IwfSBhjxYrvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Step 3: Recommend Similar Accounts via ig2vec"
      ],
      "metadata": {
        "id": "371rnt2YYuku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_accounts_from_seeds(seed_ids, top_k=5):\n",
        "    seed_vecs = embeddings[seed_ids]\n",
        "    avg_vec = torch.mean(seed_vecs, dim=0).unsqueeze(0)  # Average embedding\n",
        "\n",
        "    norms = F.normalize(embeddings, dim=1)\n",
        "    sim = torch.matmul(F.normalize(avg_vec, dim=1), norms.T).squeeze()\n",
        "\n",
        "    recommended_ids = torch.topk(sim, top_k + len(seed_ids)).indices.tolist()\n",
        "    recommended_ids = [i for i in recommended_ids if i not in seed_ids][:top_k]\n",
        "\n",
        "    print(\"üîé Recommended accounts based on seed:\")\n",
        "    for idx in recommended_ids:\n",
        "        print(f\"‚Üí {idx2word[idx]} (similarity: {sim[idx]:.4f})\")\n"
      ],
      "metadata": {
        "id": "2Y0rLnA8Yyry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß™ Example Usage:"
      ],
      "metadata": {
        "id": "ddpZbpMkY1qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_sessions = [\n",
        "    [\"catsdaily\", \"funnycats\", \"meowworld\"],  # past sessions\n",
        "    [\"catvideos\", \"meowworld\"]                # most recent session\n",
        "]\n",
        "\n",
        "# 1. Find seed accounts\n",
        "seed_ids = get_seed_accounts(user_sessions)\n",
        "\n",
        "print(f\"seed ids{seed_ids}\")\n",
        "\n",
        "# 2. Recommend similar accounts\n",
        "recommend_accounts_from_seeds(seed_ids, top_k=6)\n"
      ],
      "metadata": {
        "id": "7aaNMLsmY4cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ This mimics Instagram‚Äôs Explore sourcing step:\n",
        "\n",
        "    Recent user interactions ‚Üí seed accounts\n",
        "\n",
        "    ig2vec embeddings ‚Üí similar accounts\n",
        "\n",
        "    Media from similar accounts ‚Üí candidates for recommendation"
      ],
      "metadata": {
        "id": "g0ZQKHZNZAo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Part 1: Ranking Candidate Posts Using a Value Model\n",
        "\n",
        "Just like Instagram‚Äôs Explore system, we‚Äôll assign a score to each post based on predicted user actions.\n",
        "üéØ Value Model Formula:"
      ],
      "metadata": {
        "id": "6L005KFUZLy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example predicted probabilities for a post (you can get them from a model or simulate)\n",
        "P_like = 0.7\n",
        "P_save = 0.4\n",
        "P_hide = 0.1\n",
        "\n",
        "# Define weights\n",
        "w1 = 1.0  # weight for like\n",
        "w2 = 1.2  # weight for save\n",
        "w3 = 2.0  # penalty for hide\n",
        "\n",
        "# Compute value score\n",
        "score = w1 * P_like + w2 * P_save - w3 * P_hide\n",
        "print(f\"Final Value Score: {score:.4f}\")\n",
        "\n",
        "#or\n",
        "def value_model(P_like, P_save, P_hide, w1=1.0, w2=1.2, w3=2.0):\n",
        "    return w1 * P_like + w2 * P_save - w3 * P_hide\n",
        "\n",
        "score = value_model(P_like, P_save, P_hide)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W-mdeSvoZMih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We simulate probabilities using dummy prediction functions or random scores (in real systems, they‚Äôre outputs from neural nets)."
      ],
      "metadata": {
        "id": "vsklk_vhZOkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß™ Step 1: Simulate Candidate Posts\n",
        "\n",
        "Let‚Äôs say the accounts recommended (from ig2vec) posted the following content:"
      ],
      "metadata": {
        "id": "p5hYZCIsZRo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fake post content metadata from recommended accounts\n",
        "posts = [\n",
        "    {\"post_id\": \"p101\", \"account\": \"catvideos\"},\n",
        "    {\"post_id\": \"p102\", \"account\": \"meowworld\"},\n",
        "    {\"post_id\": \"p103\", \"account\": \"funnycats\"},\n",
        "]\n"
      ],
      "metadata": {
        "id": "vEtGIqWhZXag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üé≤ Step 2: Simulate Predicted User Actions"
      ],
      "metadata": {
        "id": "vFKvERkxZcBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_user_actions(post):\n",
        "    \"\"\"Simulated probabilities of like, save, hide\"\"\"\n",
        "    return {\n",
        "        \"P_like\": np.random.uniform(0.3, 0.9),\n",
        "        \"P_save\": np.random.uniform(0.1, 0.8),\n",
        "        \"P_hide\": np.random.uniform(0.0, 0.2)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "uE_aDGjsZc0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Step 3: Rank Candidates Using Value Model"
      ],
      "metadata": {
        "id": "oC7BQdNHZe7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_posts(posts, w_like=1.0, w_save=1.2, w_hide=2.0):\n",
        "    ranked_posts = []\n",
        "    for post in posts:\n",
        "        probs = predict_user_actions(post)\n",
        "        value_score = (\n",
        "            w_like * probs[\"P_like\"] +\n",
        "            w_save * probs[\"P_save\"] -\n",
        "            w_hide * probs[\"P_hide\"]\n",
        "        )\n",
        "        ranked_posts.append((post[\"post_id\"], post[\"account\"], value_score))\n",
        "\n",
        "    ranked_posts.sort(key=lambda x: -x[2])  # Descending order\n",
        "\n",
        "    print(\"üìà Ranked Recommended Posts:\")\n",
        "    for pid, acc, score in ranked_posts:\n",
        "        print(f\"‚Üí Post {pid} from @{acc}, Score: {score:.4f}\")\n",
        "\n",
        "    return ranked_posts  # ‚úÖ this line is the fix\n"
      ],
      "metadata": {
        "id": "dDVdjk52ZhwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ñ∂Ô∏è Example usage:"
      ],
      "metadata": {
        "id": "jSY2uAAtZkGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rank_posts(posts)\n"
      ],
      "metadata": {
        "id": "ke89G-sYZmq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üñºÔ∏è Part 2: Explore-style Grid Display (Console Simulation)\n",
        "\n",
        "Let‚Äôs simulate a basic Explore feed grid (3x3) of top posts."
      ],
      "metadata": {
        "id": "hFK8IjXDZpfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_explore_grid(ranked_posts, grid_size=3):\n",
        "    print(\"\\nüß± Instagram Explore Grid\\n\")\n",
        "    for i in range(grid_size):\n",
        "        row = ranked_posts[i * grid_size:(i + 1) * grid_size]\n",
        "        row_str = \" | \".join([f\"{pid}@{acc}\" for pid, acc, _ in row])\n",
        "        print(row_str)\n"
      ],
      "metadata": {
        "id": "WdYCP6ZEZss8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÅ Run it all together:"
      ],
      "metadata": {
        "id": "THFdqcfEZuqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_ranked = rank_posts(posts)\n",
        "show_explore_grid(top_ranked, grid_size=1)\n"
      ],
      "metadata": {
        "id": "dZ7mN_4DZxsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Part 1: Add Diversity Logic\n",
        "\n",
        "To avoid showing multiple posts from the same account, we'll add a penalty if the same account appears again in the ranking list.\n",
        "üß† Logic:\n",
        "\n",
        "    For each post, check if its account already appeared.\n",
        "\n",
        "    If yes, apply a penalty (e.g. subtract 0.2 from score).\n",
        "\n",
        "    This mimics Instagram‚Äôs \"don‚Äôt repeat author\" heuristic.\n",
        "\n",
        "üõ† Update to rank_posts() with diversity logic:"
      ],
      "metadata": {
        "id": "tTXfNIAQbf_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_posts(posts, w_like=1.0, w_save=1.2, w_hide=2.0, diversity_penalty=0.2):\n",
        "    ranked_posts = []\n",
        "    seen_accounts = set()\n",
        "\n",
        "    for post in posts:\n",
        "        probs = predict_user_actions(post)\n",
        "        score = (\n",
        "            w_like * probs[\"P_like\"] +\n",
        "            w_save * probs[\"P_save\"] -\n",
        "            w_hide * probs[\"P_hide\"]\n",
        "        )\n",
        "\n",
        "        # Apply diversity penalty if we've already seen this account\n",
        "        if post[\"account\"] in seen_accounts:\n",
        "            score -= diversity_penalty\n",
        "        else:\n",
        "            seen_accounts.add(post[\"account\"])\n",
        "\n",
        "        ranked_posts.append((post[\"post_id\"], post[\"account\"], score))\n",
        "\n",
        "    ranked_posts.sort(key=lambda x: -x[2])  # Sort descending by score\n",
        "\n",
        "    print(\"üìà Ranked Recommended Posts (with diversity penalty):\")\n",
        "    for pid, acc, score in ranked_posts:\n",
        "        print(f\"‚Üí Post {pid} from @{acc}, Score: {score:.4f}\")\n",
        "\n",
        "    return ranked_posts\n"
      ],
      "metadata": {
        "id": "WNoay5cDbg_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_ranked = rank_posts(posts)\n",
        "show_explore_grid(top_ranked, grid_size=1)"
      ],
      "metadata": {
        "id": "xNr8Pqcq698y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you'd like, we can simulate more posts, apply filtering, or build this into a streamlit or Flask demo."
      ],
      "metadata": {
        "id": "7Ogb4Z6VZzjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Part 2: Use Real Neural Network for Prediction\n",
        "\n",
        "We'll simulate a tiny neural network that predicts user actions (like/save/hide) based on post + account embeddings."
      ],
      "metadata": {
        "id": "5N7jdDugbnnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PostScorer(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 3),  # 3 outputs: like, save, hide\n",
        "            nn.Sigmoid()       # get probabilities\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "vglIyU4hbovL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Connect Account Embedding to Model:"
      ],
      "metadata": {
        "id": "8WfykUqYbtjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "scoring_model = PostScorer(embed_dim=2)\n",
        "\n",
        "def predict_user_actions_nn(post):\n",
        "    \"\"\"Use NN to predict like/save/hide\"\"\"\n",
        "    acc_id = word2idx.get(post[\"account\"])\n",
        "    if acc_id is None:\n",
        "        return {\"P_like\": 0.5, \"P_save\": 0.3, \"P_hide\": 0.1}  # default\n",
        "\n",
        "    embed = model.input_embeddings(torch.tensor([acc_id])).detach()\n",
        "    with torch.no_grad():\n",
        "        preds = scoring_model(embed).squeeze().numpy()\n",
        "\n",
        "    return {\n",
        "        \"P_like\": preds[0],\n",
        "        \"P_save\": preds[1],\n",
        "        \"P_hide\": preds[2]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "LK_giSrgbuXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÑ Replace:\n",
        "\n",
        "In rank_posts(), replace this line:"
      ],
      "metadata": {
        "id": "gaJQnHzPbxB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_posts(posts, w_like=1.0, w_save=1.2, w_hide=2.0, diversity_penalty=0.2):\n",
        "    ranked_posts = []\n",
        "    seen_accounts = set()\n",
        "\n",
        "    for post in posts:\n",
        "        probs = predict_user_actions(post)\n",
        "        score = (\n",
        "            w_like * probs[\"P_like\"] +\n",
        "            w_save * probs[\"P_save\"] -\n",
        "            w_hide * probs[\"P_hide\"]\n",
        "        )\n",
        "\n",
        "        # Apply diversity penalty if we've already seen this account\n",
        "        if post[\"account\"] in seen_accounts:\n",
        "            score -= diversity_penalty\n",
        "        else:\n",
        "            seen_accounts.add(post[\"account\"])\n",
        "\n",
        "        ranked_posts.append((post[\"post_id\"], post[\"account\"], score))\n",
        "\n",
        "    ranked_posts.sort(key=lambda x: -x[2])  # Sort descending by score\n",
        "\n",
        "    print(\"üìà Ranked Recommended Posts (with diversity penalty):\")\n",
        "    for pid, acc, score in ranked_posts:\n",
        "        print(f\"‚Üí Post {pid} from @{acc}, Score: {score:.4f}\")\n",
        "\n",
        "    return ranked_posts\n",
        "\n"
      ],
      "metadata": {
        "id": "bwXZLI3Tb0v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vKfX1toV7mrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_ranked = rank_posts(posts)\n",
        "show_explore_grid(top_ranked, grid_size=1)"
      ],
      "metadata": {
        "id": "NiT9dayF7nUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ This simulates Instagram‚Äôs setup:\n",
        "\n",
        "    Embedding from ig2vec\n",
        "\n",
        "    Feed into NN\n",
        "\n",
        "    Get probabilities of user actions\n",
        "\n",
        "    Rank using value model\n",
        "\n",
        "    Add diversity penalty"
      ],
      "metadata": {
        "id": "L3gqV5LkcVWr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j3F9UsKucWGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MC8SxPaNb3Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AWE7dkE3Yni_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}